# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10i8rwbISuiQK6Qsyu8M0qV3Fx61CB5RE
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings("ignore")

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report
from sklearn.preprocessing import StandardScaler

import lightgbm as lgb
import shap
from lime.lime_tabular import LimeTabularExplainer


# LOAD DATA
df = pd.read_csv("credit_data.csv")
target = "default"

X = df.drop(columns=[target])
y = df[target]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.25, random_state=42, stratify=y
)


# MODEL TRAINING & TUNING
lgbm = lgb.LGBMClassifier()

param_grid = {
    "num_leaves": [31, 50],
    "learning_rate": [0.05, 0.1],
    "n_estimators": [200, 400],
}

grid = GridSearchCV(lgbm, param_grid, scoring="roc_auc", cv=3, n_jobs=-1)
grid.fit(X_train, y_train)
best_model = grid.best_estimator_


# MODEL PERFORMANCE SUMMARY
y_pred = best_model.predict(X_test)
y_prob = best_model.predict_proba(X_test)[:, 1]

accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_prob)
report = classification_report(y_test, y_pred)

performance_text = (
    "=== MODEL PERFORMANCE SUMMARY ===\n"
    f"Accuracy: {accuracy:.4f}\n"
    f"ROC-AUC: {roc_auc:.4f}\n\n"
    "Classification Report:\n"
    f"{report}"
)

with open("model_performance_summary.txt", "w") as f:
    f.write(performance_text)


# GLOBAL SHAP ANALYSIS
explainer = shap.TreeExplainer(best_model)
shap_values = explainer.shap_values(X_train)

shap_importance = np.mean(np.abs(shap_values[1]), axis=0)
feature_importance = sorted(
    list(zip(X.columns, shap_importance)), key=lambda x: x[1], reverse=True
)

top10 = feature_importance[:10]

shap_text = "=== TOP 10 GLOBAL SHAP FEATURES ===\n"
for i, (feat, val) in enumerate(top10, 1):
    shap_text += f"{i}. {feat} — Impact: {val:.4f}\n"

with open("global_shap_feature_importance.txt", "w") as f:
    f.write(shap_text)


# LOCAL SHAP & LIME EXPLANATIONS
high_risk_index = np.argmax(y_prob)
low_risk_index = np.argmin(y_prob)
border_index = np.argsort(y_prob)[len(y_prob)//2]

selected = {
    "HIGH RISK CASE": high_risk_index,
    "LOW RISK CASE": low_risk_index,
    "BORDERLINE CASE": border_index
}

lime_explainer = LimeTabularExplainer(
    training_data=np.array(X_train),
    feature_names=X.columns.tolist(),
    class_names=["No Default", "Default"],
    mode="classification"
)

analysis_text = "=== SHAP vs LIME COMPARATIVE ANALYSIS ===\n"

for case_name, idx in selected.items():

    analysis_text += f"\n--- {case_name} (Sample Index: {idx}) ---\n"
    x_instance = X_test[idx]

    shap_local = explainer.shap_values(x_instance.reshape(1, -1))[1][0]
    sorted_shap = sorted(
        list(zip(X.columns, shap_local)), key=lambda x: abs(x[1]), reverse=True
    )[:8]

    analysis_text += "SHAP Top Features:\n"
    for feat, val in sorted_shap:
        direction = "↑ increases risk" if val > 0 else "↓ reduces risk"
        analysis_text += f"{feat}: {val:.4f} → {direction}\n"

    analysis_text += "\nLIME Interpretation:\n"
    lime_local = lime_explainer.explain_instance(
        data_row=x_instance,
        predict_fn=best_model.predict_proba
    )

    for feat, val in lime_local.as_list():
        analysis_text += f"{feat}: {val:.4f}\n"

    analysis_text += (
        "\nComparison Summary:\n"
        "- SHAP provides consistent, direction-aware feature contributions.\n"
        "- LIME gives intuitive, human-readable local approximations.\n"
        "- Differences may occur where LIME approximates linear boundaries while SHAP captures true nonlinear effects.\n"
    )

with open("shap_lime_comparative_analysis.txt", "w") as f:
    f.write(analysis_text)


print("All required deliverables generated successfully!")